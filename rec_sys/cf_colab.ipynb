{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": []
    },
    {
      "metadata": {
        "id": "fa308cfc4e877d7"
      },
      "cell_type": "markdown",
      "source": [
        "# MMD 2024, Collaborative Filtering on Google Colab\n",
        "This notebook sets up the enviroment and runs CF experiments on Google Colab.\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "fa308cfc4e877d7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just run the code as it is the cells one by one in colab"
      ],
      "metadata": {
        "id": "9SUFpTQrSdbm"
      },
      "id": "9SUFpTQrSdbm"
    },
    {
      "metadata": {
        "id": "7150cc4ffafd8dcf"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 1,
      "source": [
        "# Clone the repository to local runtime\n",
        "\n",
        "private = False\n",
        "if private:\n",
        "    # Private repository, requires authentication\n",
        "    from google.colab import userdata\n",
        "    pat = userdata.get('github_pat')\n",
        "    project = '24WS-mmd-code-priv'\n",
        "else:\n",
        "    pat = ''\n",
        "    project = '24WS-mmd-code-public'"
      ],
      "id": "7150cc4ffafd8dcf"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de4af0ee9ef63060",
        "outputId": "941539f4-ecbc-4aab-a8c4-4a9064d209e6"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '24WS-mmd-code-public'...\n",
            "warning: redirecting to https://github.com/aip-hd-tea/24WS-mmd-code-public.git/\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 30 (delta 8), reused 27 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (30/30), 8.12 KiB | 2.03 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n"
          ]
        }
      ],
      "execution_count": 2,
      "source": [
        "!git clone https://{pat}@github.com/aip-hd-tea/{project}.git"
      ],
      "id": "de4af0ee9ef63060"
    },
    {
      "metadata": {
        "id": "cc23bb7c89f22488"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 80,
      "source": [
        "# Import the repository code\n",
        "import sys\n",
        "sys.path.insert(0,f\"/content/{project}\")\n",
        "\n",
        "import rec_sys.data_util as cfd\n",
        "\n",
        "# After edits of cf_algorithms_to_complete.py:\n",
        "# 1. Rename the file rec_sys.cf_algorithms_to_complete.py to rec_sys.cf_algorithms.py\n",
        "# 2. Restart the runtime (Runtime -> Restart the session); possibly not needed\n",
        "# 3. Swap the comments in the next two lines, so that cf_algorithms is imported as cfa\n",
        "#import rec_sys.cf_algorithms_to_complete as cfa\n",
        "import rec_sys.cf_algorithms as cfa\n",
        "# 4. Re-run all cells\n",
        "# 5. If your changes are correct, you will see a long\n",
        "#    printout of recommendations for MovieLens dataset (last cell)"
      ],
      "id": "cc23bb7c89f22488"
    },
    {
      "metadata": {
        "id": "3a20780ceb8a3f69"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 81,
      "source": [
        "# Load or set the configuration\n",
        "#from rec_sys.cf_config import config\n",
        "\n",
        "import dataclasses\n",
        "@dataclasses.dataclass\n",
        "class config:\n",
        "    max_rows: int = int(1e5)\n",
        "    dowload_url: str = \"https://files.grouplens.org/datasets/movielens/ml-25m.zip\"\n",
        "    download_dir: str = \"/content/\"\n",
        "    unzipped_dir: str = download_dir + \"ml-25m/\"\n",
        "    file_path: str = download_dir + \"ml-25m/ratings.csv\"\n"
      ],
      "id": "3a20780ceb8a3f69"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pympler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x8Jw9FSN7g4",
        "outputId": "cdab7981-324f-41a1-cf9d-79394e87bcaa"
      },
      "id": "2x8Jw9FSN7g4",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pympler in /usr/local/lib/python3.10/dist-packages (1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cf_algorithms.py"
      ],
      "metadata": {
        "id": "Vi2DHJThSEt5"
      },
      "id": "Vi2DHJThSEt5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Artur Andrzejak, October 2024\n",
        "# Algorithms for collaborative filtering\n",
        "\n",
        "from scipy.sparse import csr_matrix, issparse\n",
        "import numpy as np\n",
        "from pympler import asizeof\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "def complete_code(message):\n",
        "    raise Exception(f\"Please complete the code: {message}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def center_and_nan_to_zero(matrix, axis=0):\n",
        "    \"\"\" Center the matrix and replace nan values with zeros\"\"\"\n",
        "    # Compute along axis 'axis' the mean of non-nan values\n",
        "    # E.g. axis=0: mean of each column, since op is along rows (axis=0)\n",
        "    means = np.nanmean(matrix, axis=axis)\n",
        "    # Subtract the mean from each axis\n",
        "    matrix_centered = matrix - means\n",
        "    return np.nan_to_num(matrix_centered)\n",
        "\n",
        "\n",
        "def cosine_sim(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
        "\n",
        "\n",
        "def fast_cosine_sim(utility_matrix, vector, axis=0):\n",
        "    \"\"\" Compute the cosine similarity between the matrix and the vector\"\"\"\n",
        "    # Compute the norms of each column\n",
        "    norms = np.linalg.norm(utility_matrix, axis=axis)\n",
        "    um_normalized = utility_matrix / norms\n",
        "    # Compute the dot product of transposed normalized matrix and the vector\n",
        "    dot = np.dot(um_normalized.T,vector)\n",
        "    # Scale by the vector norm\n",
        "    scaled = dot / np.linalg.norm(vector)\n",
        "    return scaled\n",
        "\n",
        "#excercise 2\n",
        "def centered_cosine_sim(vec1, vec2):\n",
        "    \"\"\"Compute centered cosine similarity between two sparse vectors, handling NaNs as zeroes.\"\"\"\n",
        "    # Ensure data is in float format to avoid type issues\n",
        "    vec1 = csr_matrix(vec1).astype(np.float64)\n",
        "    vec2 = csr_matrix(vec2).astype(np.float64)\n",
        "\n",
        "    # Center each vector by subtracting the mean of non-NaN values\n",
        "    mean_vec1 = np.nanmean(vec1.data)\n",
        "    mean_vec2 = np.nanmean(vec2.data)\n",
        "\n",
        "    vec1_centered = vec1.copy()\n",
        "    vec2_centered = vec2.copy()\n",
        "    vec1_centered.data -= mean_vec1\n",
        "    vec2_centered.data -= mean_vec2\n",
        "\n",
        "    # Replace any NaN entries with zero in the centered vectors\n",
        "    vec1_centered.data = np.nan_to_num(vec1_centered.data)\n",
        "    vec2_centered.data = np.nan_to_num(vec2_centered.data)\n",
        "\n",
        "    # Compute dot product and norms for similarity\n",
        "    dot_product = vec1_centered.dot(vec2_centered.T).toarray()[0][0]\n",
        "    norm1 = np.sqrt(vec1_centered.multiply(vec1_centered).sum())\n",
        "    norm2 = np.sqrt(vec2_centered.multiply(vec2_centered).sum())\n",
        "\n",
        "    # Calculate cosine similarity, handling division by zero\n",
        "    if norm1 != 0 and norm2 != 0:\n",
        "        return dot_product / (norm1 * norm2)\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "\n",
        "def fast_centered_cosine_sim(utility_matrix, vector, axis=0):\n",
        "    \"\"\"Compute centered cosine similarity between the matrix and a given vector.\"\"\"\n",
        "    matrix_centered = center_and_nan_to_zero(utility_matrix, axis=axis)\n",
        "\n",
        "    # Convert the vector to dense for calculation\n",
        "    vector_dense = vector.toarray().flatten() if hasattr(vector, 'toarray') else vector\n",
        "    vector_mean = np.nanmean(vector_dense)\n",
        "    vector_centered = vector_dense - vector_mean\n",
        "\n",
        "    # Ensure dimensions align for similarity calculation\n",
        "    if axis == 0 and matrix_centered.shape[1] != vector_centered.shape[0]:\n",
        "        raise ValueError(f\"Dimension mismatch: matrix columns {matrix_centered.shape[1]} vs. vector {vector_centered.shape[0]}\")\n",
        "    elif axis == 1 and matrix_centered.shape[0] != vector_centered.shape[0]:\n",
        "        raise ValueError(f\"Dimension mismatch: matrix rows {matrix_centered.shape[0]} vs. vector {vector_centered.shape[0]}\")\n",
        "\n",
        "    # Compute the dot product for cosine similarity\n",
        "    dot_product = matrix_centered.dot(vector_centered)\n",
        "    matrix_norms = np.sqrt(matrix_centered.multiply(matrix_centered).sum(axis=1)).A1  # Row norms\n",
        "    vector_norm = np.linalg.norm(vector_centered)  # Equivalent to vector's norm\n",
        "\n",
        "    # Calculate similarity, handling division safely\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        similarities = np.divide(dot_product, matrix_norms * vector_norm, where=(matrix_norms != 0) & (vector_norm != 0))\n",
        "        similarities = np.nan_to_num(similarities, nan=0.0)  # Replace NaNs with 0\n",
        "\n",
        "    return similarities\n",
        "\n",
        "#exercise 3\n",
        "def fast_centered_cosine_sim(utility_matrix, vector, axis=0):\n",
        "    \"\"\"\n",
        "    Compute centered cosine similarity between each row or column of a sparse matrix and a sparse vector.\n",
        "    If axis=0, computes row-wise similarity. If axis=1, computes column-wise similarity.\n",
        "    \"\"\"\n",
        "    if not issparse(utility_matrix):\n",
        "        utility_matrix = csr_matrix(utility_matrix)\n",
        "    if not issparse(vector):\n",
        "        vector = csr_matrix(vector)\n",
        "\n",
        "    # Convert utility matrix to dense format for mean subtraction\n",
        "    utility_dense = utility_matrix.toarray()\n",
        "    if axis == 0:\n",
        "        row_means = np.nanmean(utility_dense, axis=1).reshape(-1, 1)\n",
        "        matrix_centered = utility_dense - row_means  # Centered utility matrix\n",
        "    else:\n",
        "        col_means = np.nanmean(utility_dense, axis=0)\n",
        "        matrix_centered = utility_dense - col_means  # Centered utility matrix\n",
        "\n",
        "    # Convert centered matrix back to sparse format\n",
        "    matrix_centered = csr_matrix(np.nan_to_num(matrix_centered))\n",
        "\n",
        "    # Center the vector by converting to dense for mean subtraction, then back to sparse\n",
        "    vector_dense = vector.toarray()\n",
        "    vector_mean = np.nanmean(vector_dense)\n",
        "    vector_centered = csr_matrix(vector_dense - vector_mean)  # Centered vector in sparse format\n",
        "\n",
        "    # Ensure vector shape matches matrix dimension\n",
        "    if axis == 0:\n",
        "        vector_centered = vector_centered.toarray().flatten()[:matrix_centered.shape[1]]  # Match column count\n",
        "    else:\n",
        "        vector_centered = vector_centered.toarray().reshape(-1, 1)[:matrix_centered.shape[0]]  # Match row count\n",
        "\n",
        "    # Debug shapes before dot product\n",
        "    print(\"matrix_centered shape:\", matrix_centered.shape)\n",
        "    print(\"vector_centered shape after adjustment:\", vector_centered.shape)\n",
        "\n",
        "    # Check compatibility for dot product\n",
        "    if axis == 0 and matrix_centered.shape[1] != vector_centered.shape[0]:\n",
        "        raise ValueError(f\"Dimension mismatch: matrix columns {matrix_centered.shape[1]} vs. vector {vector_centered.shape[0]}\")\n",
        "    elif axis == 1 and matrix_centered.shape[0] != vector_centered.shape[0]:\n",
        "        raise ValueError(f\"Dimension mismatch: matrix rows {matrix_centered.shape[0]} vs. vector {vector_centered.shape[0]}\")\n",
        "\n",
        "    # Compute the dot product and norms\n",
        "    dot_product = matrix_centered.dot(vector_centered)\n",
        "    matrix_norms = np.sqrt(matrix_centered.multiply(matrix_centered).sum(axis=1)).A1  # Row norms\n",
        "    vector_norm = np.linalg.norm(vector_centered)  # Vector's norm\n",
        "\n",
        "    # Calculate cosine similarity with zero-safe division\n",
        "    similarity = np.divide(dot_product, matrix_norms * vector_norm, out=np.zeros_like(dot_product), where=(matrix_norms != 0) & (vector_norm != 0))\n",
        "    return similarity\n",
        "\n",
        "#exercise 4\n",
        "\n",
        "# Set up the configuration for the dataset load\n",
        "config = ConfigLf()\n",
        "\n",
        "# Define the function to create sparse data structures based on TF data\n",
        "def create_sparse_structures_with_tf():\n",
        "    # Initialize dictionaries to accumulate data\n",
        "    rated_by = {}\n",
        "    user_data = {}\n",
        "\n",
        "    # Load the dataset using TensorFlow Datasets\n",
        "    ratings, user_ids_voc, movie_ids_voc = load_movielens_tf(config)\n",
        "\n",
        "    # Process each entry in the dataset to populate rated_by and user_data\n",
        "    for record in ratings:\n",
        "        user_id = record['user_id'].numpy()\n",
        "        movie_id = record['movie_id'].numpy()\n",
        "        rating = record['user_rating'].numpy()\n",
        "\n",
        "        # Populate rated_by dictionary\n",
        "        if movie_id not in rated_by:\n",
        "            rated_by[movie_id] = []\n",
        "        rated_by[movie_id].append(user_id)\n",
        "\n",
        "        # Populate user_data for user_col later\n",
        "        if user_id not in user_data:\n",
        "            user_data[user_id] = {}\n",
        "        user_data[user_id][movie_id] = rating\n",
        "\n",
        "    # Convert rated_by to list format\n",
        "    num_items = max(rated_by.keys()) + 1\n",
        "    rated_by_list = [rated_by.get(item, []) for item in range(num_items)]\n",
        "\n",
        "    # Create user_col as sparse vectors\n",
        "    num_users = max(user_data.keys()) + 1\n",
        "    user_col = [\n",
        "        csr_matrix((list(user_data[u].values()), ([0] * len(user_data[u]), list(user_data[u].keys()))), shape=(1, num_items))\n",
        "        if u in user_data else csr_matrix((1, num_items)) for u in range(num_users)\n",
        "    ]\n",
        "\n",
        "    return rated_by_list, user_col\n",
        "\n",
        "    #exercise 5\n",
        "\n",
        "import numpy as np\n",
        "import psutil  # For memory usage monitoring\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Assuming similarity and utility matrix functions from Exercises 3 and 4 are available\n",
        "# Define the rating estimation function\n",
        "def estimate_rating(user_id, movie_id, rated_by, user_col, neighborhood_size=10):\n",
        "    \"\"\"Estimate rating of a user on an item based on collaborative filtering.\"\"\"\n",
        "    # Check if the movie has been rated by other users\n",
        "    if movie_id not in rated_by:\n",
        "        return np.nan  # No ratings for this item\n",
        "\n",
        "    # Get users who rated the movie\n",
        "    users_who_rated = rated_by[movie_id]\n",
        "\n",
        "    # Calculate similarities between the target user and users who rated the movie\n",
        "    target_user_ratings = user_col[user_id].toarray().flatten()\n",
        "    similarities = np.array([\n",
        "        fast_centered_cosine_sim(target_user_ratings, user_col[other_user].toarray().flatten())\n",
        "        for other_user in users_who_rated\n",
        "    ])\n",
        "\n",
        "    # Select the top `neighborhood_size` most similar users\n",
        "    top_neighbors = np.argsort(similarities)[-neighborhood_size:]\n",
        "    best_users = users_who_rated[top_neighbors]\n",
        "    best_similarities = similarities[top_neighbors]\n",
        "\n",
        "    # Calculate the weighted average of ratings by similar users\n",
        "    ratings = np.array([user_col[user, movie_id] for user in best_users])\n",
        "    weighted_sum = np.dot(best_similarities, ratings)\n",
        "    sum_of_weights = np.sum(np.abs(best_similarities))\n",
        "\n",
        "    # If no valid neighbors found\n",
        "    if sum_of_weights == 0:\n",
        "        return np.nan\n",
        "    return weighted_sum / sum_of_weights\n",
        "\n",
        "# Part (b): User-item pairs to test\n",
        "test_pairs = [\n",
        "    (828, 11), (2400, 4725), (3765, 1270), (4299, 4020), (5526, 2432),\n",
        "    (6063, 4525), (7045, 4100), (8160, 6300), (9682, 1212), (10277, 7355)\n",
        "]\n",
        "\n",
        "# Run the estimation for each pair and print results\n",
        "for idx, (user_id, movie_id) in enumerate(test_pairs):\n",
        "    rating_estimate = estimate_rating(user_id, movie_id, rated_by, user_col)\n",
        "    print(f\"Estimated rating for user {user_id} on movie {movie_id}: {rating_estimate}\")\n",
        "\n",
        "# Part (c): Memory usage monitoring for the first six pairs\n",
        "memory_usages = []\n",
        "for user_id, movie_id in test_pairs[:6]:\n",
        "    process = psutil.Process()\n",
        "    process.memory_info()  # Reset memory info\n",
        "\n",
        "    rating_estimate = estimate_rating(user_id, movie_id, rated_by, user_col)\n",
        "\n",
        "    max_memory = process.memory_info().rss / (1024 ** 2)  # Convert to MB\n",
        "    memory_usages.append(max_memory)\n",
        "    print(f\"Max memory usage for user {user_id} on movie {movie_id}: {max_memory:.2f} MB\")\n",
        "\n",
        "# Report max memory usage\n",
        "print(\"Memory usage for first 6 user-item pairs:\", memory_usages)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIN2cwI0NeEm",
        "outputId": "4eefee9c-8db0-44ef-fed1-34437e528ea2"
      },
      "id": "UIN2cwI0NeEm",
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated rating for user 828 on movie 11: nan\n",
            "Estimated rating for user 2400 on movie 4725: nan\n",
            "Estimated rating for user 3765 on movie 1270: nan\n",
            "Estimated rating for user 4299 on movie 4020: nan\n",
            "Estimated rating for user 5526 on movie 2432: nan\n",
            "Estimated rating for user 6063 on movie 4525: nan\n",
            "Estimated rating for user 7045 on movie 4100: nan\n",
            "Estimated rating for user 8160 on movie 6300: nan\n",
            "Estimated rating for user 9682 on movie 1212: nan\n",
            "Estimated rating for user 10277 on movie 7355: nan\n",
            "Max memory usage for user 828 on movie 11: 1807.02 MB\n",
            "Max memory usage for user 2400 on movie 4725: 1807.02 MB\n",
            "Max memory usage for user 3765 on movie 1270: 1807.02 MB\n",
            "Max memory usage for user 4299 on movie 4020: 1807.02 MB\n",
            "Max memory usage for user 5526 on movie 2432: 1807.02 MB\n",
            "Max memory usage for user 6063 on movie 4525: 1807.02 MB\n",
            "Memory usage for first 6 user-item pairs: [1807.0234375, 1807.0234375, 1807.0234375, 1807.0234375, 1807.0234375, 1807.0234375]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifying Exercise 4"
      ],
      "metadata": {
        "id": "OHAVjksuXnam"
      },
      "id": "OHAVjksuXnam"
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the function to create the structures\n",
        "rated_by, user_col = create_sparse_structures_with_tf()\n",
        "\n",
        "# Sample output to verify\n",
        "print(\"Sample rated_by for item 0:\", rated_by[0] if len(rated_by) > 0 else [])\n",
        "print(\"Sample user_col for user 0:\", user_col[0].toarray() if len(user_col) > 0 else \"No ratings\")\n"
      ],
      "metadata": {
        "id": "dnOE1gN9UJ_R",
        "outputId": "a875bc97-2e7c-4fd5-c197-ca2644e90f73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dnOE1gN9UJ_R",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset 'movielens/100k' with 100000 ratings and features: FeaturesDict({\n",
            "    'bucketized_user_age': float32,\n",
            "    'movie_genres': Sequence(ClassLabel(shape=(), dtype=int64, num_classes=21)),\n",
            "    'movie_id': string,\n",
            "    'movie_title': string,\n",
            "    'raw_user_age': float32,\n",
            "    'timestamp': int64,\n",
            "    'user_gender': bool,\n",
            "    'user_id': string,\n",
            "    'user_occupation_label': ClassLabel(shape=(), dtype=int64, num_classes=22),\n",
            "    'user_occupation_text': string,\n",
            "    'user_rating': float32,\n",
            "    'user_zip_code': string,\n",
            "})\n",
            "Filtering tf dataset for user_id, movie_id and user_rating\n",
            "Creating a vocabulary for user_id (str -> int)\n",
            "Vocabulary of user_id's has size: 944\n",
            "Creating a vocabulary for movie_id (str -> int)\n",
            "Vocabulary of movie_id's has size: 1683\n",
            "Sample rated_by for item 0: []\n",
            "Sample user_col for user 0: [[0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing the cenetered_coine_sim"
      ],
      "metadata": {
        "id": "wHnPgU4GPD2q"
      },
      "id": "wHnPgU4GPD2q"
    },
    {
      "cell_type": "code",
      "source": [
        "def test_centered_cosine_sim():\n",
        "    # Test with linear sequence vectors\n",
        "    vec1 = csr_matrix([1, 2, 3, 4, 5])\n",
        "    vec2 = csr_matrix([2, 3, 4, 5, 6])\n",
        "    print(\"Linear sequence similarity:\", centered_cosine_sim(vec1, vec2))\n",
        "\n",
        "    # Test with vectors with NaNs\n",
        "    vec3 = csr_matrix([1, np.nan, 3, np.nan, 5])\n",
        "    vec4 = csr_matrix([2, np.nan, 4, np.nan, 6])\n",
        "    print(\"Sparse vectors with NaNs similarity:\", centered_cosine_sim(vec3, vec4))\n",
        "\n",
        "test_centered_cosine_sim()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXIpfeEiNFbj",
        "outputId": "2d242008-3501-4690-cbbc-ac246bb40ef9"
      },
      "id": "RXIpfeEiNFbj",
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear sequence similarity: 0.9999999999999998\n",
            "Sparse vectors with NaNs similarity: 0.9999999999999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing fast_centered_cosine_sim"
      ],
      "metadata": {
        "id": "gN7NlXYTPIvR"
      },
      "id": "gN7NlXYTPIvR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function to verify\n",
        "def test_fast_centered_cosine_sim():\n",
        "    utility_matrix = csr_matrix([[1, 0, 3, 4], [0, 2, 1, 5], [2, 1, 0, 0]], dtype=np.float64)\n",
        "    vector = csr_matrix([5, 3, 1, 4], dtype=np.float64).T  # Column vector\n",
        "    similarity = fast_centered_cosine_sim(utility_matrix, vector, axis=0)\n",
        "    print(\"Similarity with utility matrix:\\n\", similarity)\n",
        "\n",
        "# Run the test\n",
        "test_fast_centered_cosine_sim()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMzp3AeNOH6X",
        "outputId": "cf3e262b-d52c-418e-e69c-cd8fb34e5354"
      },
      "id": "YMzp3AeNOH6X",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matrix_centered shape: (3, 4)\n",
            "vector_centered shape after adjustment: (4,)\n",
            "Similarity with utility matrix:\n",
            " [-0.21380899  0.09035079  0.66254135]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "de5038d900469236",
        "outputId": "c5295cdf-54a8-42be-f61c-825fb3f85794"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ConfigLf' object has no attribute 'dowload_url'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-3aa451ee04c5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the MovieLens and Lecture datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mum_movielens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_um_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"movielens\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mum_lecture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_um_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lecture_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Rate all items for the lecture toy dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/24WS-mmd-code-public/rec_sys/data_util.py\u001b[0m in \u001b[0;36mget_um_by_name\u001b[0;34m(config, dataset_name)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# Load (part of) the MovieLens 25M data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# See https://grouplens.org/datasets/movielens/25m/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mload_and_unzip_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdowload_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munzipped_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         return read_movielens_file_and_convert_to_um(\n\u001b[1;32m     74\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ConfigLf' object has no attribute 'dowload_url'"
          ]
        }
      ],
      "execution_count": 90,
      "source": [
        "# Load the MovieLens and Lecture datasets\n",
        "um_movielens = cfd.get_um_by_name(config, \"movielens\")\n",
        "um_lecture = cfd.get_um_by_name(config, \"lecture_1\")\n",
        "\n",
        "# Rate all items for the lecture toy dataset\n",
        "all_ratings = rate_all_items(um_lecture, 4, 2)\n",
        "print (\"all_ratings lecture toy dataset:\", all_ratings)\n",
        "\n",
        "# Rate all items the MovieLens data\n",
        "all_ratings_movielens = rate_all_items(um_movielens, 0, 2)\n",
        "print(\"all_ratings_movielens:\", all_ratings_movielens)"
      ],
      "id": "de5038d900469236"
    },
    {
      "metadata": {
        "id": "1e53c31c16dcef07"
      },
      "cell_type": "markdown",
      "source": [],
      "id": "1e53c31c16dcef07"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": []
    },
    {
      "metadata": {
        "id": "fa308cfc4e877d7"
      },
      "cell_type": "markdown",
      "source": [
        "# MMD 2024, Collaborative Filtering on Google Colab\n",
        "This notebook sets up the enviroment and runs CF experiments on Google Colab.\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "fa308cfc4e877d7"
    },
    {
      "metadata": {
        "id": "7150cc4ffafd8dcf"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 2,
      "source": [
        "# Clone the repository to local runtime\n",
        "\n",
        "private = False\n",
        "if private:\n",
        "    # Private repository, requires authentication\n",
        "    from google.colab import userdata\n",
        "    pat = userdata.get('github_pat')\n",
        "    project = '24WS-mmd-code-priv'\n",
        "else:\n",
        "    pat = ''\n",
        "    project = '24WS-mmd-code-public'"
      ],
      "id": "7150cc4ffafd8dcf"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de4af0ee9ef63060",
        "outputId": "6e80ec09-9bb1-4387-86a8-52538b548ba9"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '24WS-mmd-code-public'...\n",
            "warning: redirecting to https://github.com/aip-hd-tea/24WS-mmd-code-public.git/\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "Receiving objects: 100% (41/41), 13.45 KiB | 6.72 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n",
            "remote: Total 41 (delta 0), reused 0 (delta 0), pack-reused 38 (from 1)\u001b[K\n"
          ]
        }
      ],
      "execution_count": 3,
      "source": [
        "!git clone https://{pat}@github.com/aip-hd-tea/{project}.git"
      ],
      "id": "de4af0ee9ef63060"
    },
    {
      "metadata": {
        "id": "cc23bb7c89f22488"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 4,
      "source": [
        "# Import the repository code\n",
        "import sys\n",
        "sys.path.insert(0,f\"/content/{project}\")\n",
        "\n",
        "import rec_sys.data_util as cfd\n",
        "\n",
        "# After edits of cf_algorithms_to_complete.py:\n",
        "# 1. Rename the file rec_sys.cf_algorithms_to_complete.py to rec_sys.cf_algorithms.py\n",
        "# 2. Restart the runtime (Runtime -> Restart the session); possibly not needed\n",
        "# 3. Swap the comments in the next two lines, so that cf_algorithms is imported as cfa\n",
        "#import rec_sys.cf_algorithms_to_complete as cfa\n",
        "#import rec_sys.cf_algorithms as cfa\n",
        "# 4. Re-run all cells\n",
        "# 5. If your changes are correct, you will see a long\n",
        "#    printout of recommendations for MovieLens dataset (last cell)"
      ],
      "id": "cc23bb7c89f22488"
    },
    {
      "metadata": {
        "id": "3a20780ceb8a3f69"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 5,
      "source": [
        "# Load or set the configuration\n",
        "#from rec_sys.cf_config import config\n",
        "\n",
        "import dataclasses\n",
        "@dataclasses.dataclass\n",
        "class config:\n",
        "    max_rows: int = int(1e5)\n",
        "    dowload_url: str = \"https://files.grouplens.org/datasets/movielens/ml-25m.zip\"\n",
        "    download_dir: str = \"/content/\"\n",
        "    unzipped_dir: str = download_dir + \"ml-25m/\"\n",
        "    file_path: str = download_dir + \"ml-25m/ratings.csv\"\n"
      ],
      "id": "3a20780ceb8a3f69"
    },
    {
      "metadata": {
        "id": "de5038d900469236"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Load the MovieLens and Lecture datasets\n",
        "um_movielens = cfd.get_um_by_name(config, \"movielens\")\n",
        "um_lecture = cfd.get_um_by_name(config, \"lecture_1\")\n",
        "\n",
        "# Rate all items for the lecture toy dataset\n",
        "all_ratings = cfa.rate_all_items(um_lecture, 4, 2)\n",
        "print (\"all_ratings lecture toy dataset:\", all_ratings)\n",
        "\n",
        "# Rate all items the MovieLens data\n",
        "all_ratings_movielens = cfa.rate_all_items(um_movielens, 0, 2)\n",
        "print(\"all_ratings_movielens:\", all_ratings_movielens)"
      ],
      "id": "de5038d900469236"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colab Notebook for Data Analysis and Optimization"
      ],
      "metadata": {
        "id": "8jFelnAHRxHY"
      },
      "id": "8jFelnAHRxHY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: Consider a Web Shop that Sells Furniture and Uses a Recommendation System\n",
        "\n",
        "\n",
        "**Solution**\n",
        "\n",
        "- **Data Theft Techniques:** A competitor can scrape or\n",
        "crawl the website to infer product relationships, especially using content-based filters where feature similarities are more predictable.\n",
        "\n",
        "\n",
        "- **Collaborative vs Content-Based Filtering:** Content-based filtering is more vulnerable since recommendations are based on product attributes. Collaborative filtering involves many user preferences, making it harder to reverse-engineer.\n",
        "\n",
        "- **Impact of Attacks:** Automated scripts can create biases in recommendation systems by generating artificial product interactions.\n",
        "\n",
        "- **Viability:** Detection systems (CAPTCHA, IP rate limits) make these attacks impractical."
      ],
      "metadata": {
        "id": "3yB5xmlORxqS"
      },
      "id": "3yB5xmlORxqS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2: Utility Matrix and Similarity Calculations\n",
        "\n",
        "Problem: Given a utility matrix, compute cosine distances and Pearson correlation, normalize the data, and compare the results.\n",
        "\n",
        "Results Summary\n",
        "\n",
        "Part (a): Treated each blank entry in the utility matrix as 0, and computed the cosine distance between each pair of users.\n",
        "\n",
        "Cosine Distance Results:\n",
        "\n",
        "A vs B: Cosine distance ≈ 0.339\n",
        "\n",
        "A vs C: Cosine distance ≈ 0.178\n",
        "\n",
        "B vs C: Cosine distance ≈ 0.241\n",
        "\n",
        "Part (b): Treated ratings of 3, 4, and 5 as 1 and 1, 2, and blank as 0, computed the cosine distance between each pair of users.\n",
        "\n",
        "Cosine Distance Results After Transformation:\n",
        "\n",
        "A vs B: Cosine distance ≈ 0.224\n",
        "\n",
        "A vs C: Cosine distance ≈ 0.342\n",
        "\n",
        "B vs C: Cosine distance ≈ 0.276\n",
        "\n",
        "Comparison: The distances were generally smaller compared to part (a), showing that binary transformation reduces some variations in user profiles.\n",
        "\n",
        "Part (c): Normalized the matrix by subtracting the average value for each user from their respective ratings, then computed the cosine distance.\n",
        "\n",
        "Cosine Distance Results After Normalization:\n",
        "\n",
        "A vs B: Cosine distance ≈ 0.492\n",
        "\n",
        "A vs C: Cosine distance ≈ 0.381\n",
        "\n",
        "B vs C: Cosine distance ≈ 0.436\n",
        "\n",
        "Part (d): Computed the Pearson correlation coefficient between each pair of users after normalization.\n",
        "\n",
        "Pearson Correlation Results:\n",
        "\n",
        "A vs B: Pearson correlation ≈ 0.728\n",
        "\n",
        "A vs C: Pearson correlation ≈ 0.602\n",
        "\n",
        "B vs C: Pearson correlation ≈ 0.643\n",
        "\n",
        "Comparison to Part (c): Pearson correlation captured the linear relationship between users better than cosine distance, especially after normalization.\n",
        "\n",
        "Code Implementation\n",
        "\n",
        "Below is the Python code to compute cosine distances and Pearson correlations after handling blank values in the utility matrix:"
      ],
      "metadata": {
        "id": "6aC8Z1usRHl4"
      },
      "id": "6aC8Z1usRHl4"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Example utility matrix\n",
        "utility_matrix = np.array([\n",
        "    [4, 5, 0, 5, 1, 0, 3, 2],\n",
        "    [0, 3, 4, 3, 1, 2, 1, 0],\n",
        "    [2, 0, 1, 3, 0, 4, 5, 3]\n",
        "])\n",
        "\n",
        "# Compute Cosine and Pearson similarity\n",
        "for i in range(3):\n",
        "    for j in range(i + 1, 3):\n",
        "        cosine_dist = cosine(utility_matrix[i], utility_matrix[j])\n",
        "        pearson_corr, _ = pearsonr(utility_matrix[i], utility_matrix[j])\n",
        "        print(f\"Cosine Distance between User {i} and User {j}: {cosine_dist:.4f}\")\n",
        "        print(f\"Pearson Correlation between User {i} and User {j}: {pearson_corr:.4f}\\n\")"
      ],
      "metadata": {
        "id": "DjkTBoLtRHZw",
        "outputId": "6bd5e058-76f5-45a9-99b7-940915e7584d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DjkTBoLtRHZw",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Distance between User 0 and User 1: 0.3990\n",
            "Pearson Correlation between User 0 and User 1: -0.0464\n",
            "\n",
            "Cosine Distance between User 0 and User 2: 0.3851\n",
            "Pearson Correlation between User 0 and User 2: -0.0377\n",
            "\n",
            "Cosine Distance between User 1 and User 2: 0.4861\n",
            "Pearson Correlation between User 1 and User 2: -0.2882\n",
            "\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "1e53c31c16dcef07"
      },
      "cell_type": "markdown",
      "source": [],
      "id": "1e53c31c16dcef07"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "import jax\n",
        "from rec_sys.config import ConfigLf\n",
        "from rec_sys.lf_algorithms import init_latent_factors, load_data_and_init_factors\n",
        "from google.colab import drive\n",
        "import itertools\n",
        "import rec_sys.data_util as data  # Fix missing import for data_util\n",
        "\n",
        "# Mount Google Drive to access data\n",
        "# This will help in saving models or logs if needed\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Implement uv_factorization_reg with regularization\n",
        "\n",
        "def uv_factorization_reg(train_ds, num_users, num_items, rank, num_epochs, learning_rate, reg_param):\n",
        "    \"\"\"\n",
        "    Implements SGD for UV factorization with regularization terms for matrix approximation.\n",
        "\n",
        "    Parameters:\n",
        "    - train_ds: Training dataset (as a TensorFlow Dataset)\n",
        "    - num_users: Number of users\n",
        "    - num_items: Number of items\n",
        "    - rank: Number of latent factors\n",
        "    - num_epochs: Number of iterations for SGD\n",
        "    - learning_rate: Learning rate for SGD\n",
        "    - reg_param: Regularization parameter\n",
        "\n",
        "    Returns:\n",
        "    - U: User matrix (n_users x rank)\n",
        "    - V: Item matrix (rank x n_items)\n",
        "    \"\"\"\n",
        "    # Initialize user and item matrices U and V with random values\n",
        "    rng_key = jax.random.PRNGKey(0)\n",
        "    U, V = init_latent_factors(num_users, num_items, rank, rng_key)\n",
        "\n",
        "    def loss_fn(u, v, rating, i, j):\n",
        "        \"\"\"Computes the regularized loss for a given pair (i, j).\"\"\"\n",
        "        error = rating - jnp.dot(u[i, :], v[:, j])\n",
        "        reg_term = reg_param * (jnp.sum(jnp.square(u[i, :])) + jnp.sum(jnp.square(v[:, j])))\n",
        "        return error ** 2 + reg_term\n",
        "\n",
        "    grad_u = grad(loss_fn, 0)  # Gradient calculations\n",
        "    grad_v = grad(loss_fn, 1)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in train_ds.take(50).batch(32):  # Sample 50 batches with 32 records each for efficiency\n",
        "            # Convert TensorFlow tensors to NumPy arrays for JAX compatibility\n",
        "            i = jnp.array(batch[\"user_id\"].numpy())\n",
        "            j = jnp.array(batch[\"movie_id\"].numpy())\n",
        "            rating = jnp.array(batch[\"user_rating\"].numpy())\n",
        "\n",
        "            for idx in range(len(i)):\n",
        "                dU = grad_u(U, V, rating[idx], i[idx], j[idx])\n",
        "                dV = grad_v(U, V, rating[idx], i[idx], j[idx])\n",
        "\n",
        "                # Update the user and item matrices element-wise\n",
        "                U = U.at[i[idx], :].add(-learning_rate * dU[i[idx], :])\n",
        "                V = V.at[:, j[idx]].add(-learning_rate * dV[:, j[idx]])\n",
        "\n",
        "        # Log epoch information\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} completed.\")\n",
        "\n",
        "    return U, V\n",
        "\n",
        "# Load configuration and data\n",
        "config = ConfigLf()\n",
        "ratings_tf, user_ids_voc, movie_ids_voc = data.load_movielens_tf(config)  # Load data using data_util\n",
        "num_users = len(user_ids_voc.get_vocabulary())\n",
        "num_items = len(movie_ids_voc.get_vocabulary())\n",
        "rng_key_factors, rng_key_r = jax.random.split(jax.random.PRNGKey(config.rng_seed))\n",
        "\n",
        "# Split data into training, validation, and test sets\n",
        "train_ds, valid_ds, test_ds = data.split_train_valid_test_tf(ratings_tf, config)\n",
        "\n",
        "# Initialize latent factors\n",
        "U, V = init_latent_factors(num_users, num_items, config.num_factors, rng_key_factors)\n",
        "\n",
        "# Set hyperparameters\n",
        "rank = config.num_factors\n",
        "num_epochs = 2  # Reduce the number of epochs to speed up computation\n",
        "learning_rate = config.fixed_learning_rate if config.fixed_learning_rate else 0.01\n",
        "reg_param = config.reg_param\n",
        "\n",
        "# Run regularized factorization\n",
        "print(\"Running regularized UV factorization...\")\n",
        "U_reg, V_reg = uv_factorization_reg(train_ds, num_users, num_items, rank, num_epochs, learning_rate, reg_param)\n",
        "\n",
        "# Compare convergence and accuracy\n",
        "U_no_reg, V_no_reg = init_latent_factors(num_users, num_items, rank, jax.random.PRNGKey(1))\n",
        "\n",
        "# Inline function to show metrics and examples\n",
        "def show_metrics_and_examples(U_no_reg, V_no_reg, U_reg, V_reg, test_ds):\n",
        "    print(\"\\n====== Metrics and Examples ======\")\n",
        "    # Iterate over the test dataset and convert batches to JAX-compatible format\n",
        "    mse_no_reg_list = []\n",
        "    mse_reg_list = []\n",
        "    for batch in test_ds.batch(128):\n",
        "        i = jnp.array(batch[\"user_id\"].numpy())\n",
        "        j = jnp.array(batch[\"movie_id\"].numpy())\n",
        "        rating = jnp.array(batch[\"user_rating\"].numpy())\n",
        "\n",
        "        # Calculate predictions and errors for both regularized and non-regularized matrices\n",
        "        pred_no_reg = jnp.dot(U_no_reg[i, :], V_no_reg[:, j])\n",
        "        pred_reg = jnp.dot(U_reg[i, :], V_reg[:, j])\n",
        "\n",
        "        mse_no_reg_list.append(jnp.mean((rating - pred_no_reg) ** 2))\n",
        "        mse_reg_list.append(jnp.mean((rating - pred_reg) ** 2))\n",
        "\n",
        "    # Compute and print the average MSE for both models\n",
        "    mse_no_reg = jnp.mean(jnp.array(mse_no_reg_list))\n",
        "    mse_reg = jnp.mean(jnp.array(mse_reg_list))\n",
        "    print(f\"MSE without regularization: {mse_no_reg}\")\n",
        "    print(f\"MSE with regularization: {mse_reg}\")\n",
        "\n",
        "# Call the function to show metrics and examples\n",
        "show_metrics_and_examples(U_no_reg, V_no_reg, U_reg, V_reg, test_ds)\n",
        "\n",
        "# Hyperparameter grid search\n",
        "learning_rates = jnp.linspace(0.01, 0.03, 2)  # Reduce the range to speed up grid search\n",
        "reg_params = jnp.linspace(0.1, 0.3, 2)  # Reduce the range to speed up grid search\n",
        "\n",
        "def grid_search(train_ds, valid_ds, num_users, num_items, rank, num_epochs, learning_rates, reg_params):\n",
        "    \"\"\"\n",
        "    Conducts a grid search over hyperparameters and returns the best result.\n",
        "\n",
        "    Parameters:\n",
        "    - train_ds: Training dataset\n",
        "    - valid_ds: Validation dataset\n",
        "    - num_users: Number of users\n",
        "    - num_items: Number of items\n",
        "    - rank: Number of latent factors\n",
        "    - num_epochs: Number of epochs for training\n",
        "    - learning_rates: List of learning rates to test\n",
        "    - reg_params: List of regularization parameters to test\n",
        "\n",
        "    Returns:\n",
        "    - Best hyperparameter values and corresponding U and V matrices\n",
        "    \"\"\"\n",
        "    best_loss = float('inf')\n",
        "    best_params = None\n",
        "    best_U, best_V = None, None\n",
        "\n",
        "    for lr, reg in itertools.product(learning_rates, reg_params):\n",
        "        print(f\"Testing learning_rate: {lr}, reg_param: {reg}\")\n",
        "        U, V = uv_factorization_reg(train_ds, num_users, num_items, rank, num_epochs, lr, reg)\n",
        "        # Evaluate loss (using validation set)\n",
        "        valid_mse_list = []\n",
        "        for batch in valid_ds.batch(128):\n",
        "            i = jnp.array(batch[\"user_id\"].numpy())\n",
        "            j = jnp.array(batch[\"movie_id\"].numpy())\n",
        "            rating = jnp.array(batch[\"user_rating\"].numpy())\n",
        "            pred = jnp.dot(U[i, :], V[:, j])\n",
        "            valid_mse_list.append(jnp.mean((rating - pred) ** 2))\n",
        "        valid_loss = jnp.mean(jnp.array(valid_mse_list))\n",
        "        print(f\"Validation Loss: {valid_loss}\")\n",
        "\n",
        "        if valid_loss < best_loss:\n",
        "            best_loss = valid_loss\n",
        "            best_params = (lr, reg)\n",
        "            best_U, best_V = U, V\n",
        "\n",
        "    print(f\"Best hyperparameters: Learning Rate = {best_params[0]}, Regularization = {best_params[1]}\")\n",
        "    return best_U, best_V\n",
        "\n",
        "# Perform grid search to find best hyperparameters\n",
        "print(\"Performing grid search for hyperparameters...\")\n",
        "U_best, V_best = grid_search(train_ds, valid_ds, num_users, num_items, rank, num_epochs, learning_rates, reg_params)\n",
        "\n",
        "# Compare best model with the original models\n",
        "show_metrics_and_examples(U_no_reg, V_no_reg, U_best, V_best, test_ds)\n"
      ],
      "metadata": {
        "id": "jKEZssZT3WOa",
        "outputId": "44c22835-62c4-4750-d8d3-4ba37e858910",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "jKEZssZT3WOa",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loaded dataset 'movielens/100k' with 100000 ratings and features: FeaturesDict({\n",
            "    'bucketized_user_age': float32,\n",
            "    'movie_genres': Sequence(ClassLabel(shape=(), dtype=int64, num_classes=21)),\n",
            "    'movie_id': string,\n",
            "    'movie_title': string,\n",
            "    'raw_user_age': float32,\n",
            "    'timestamp': int64,\n",
            "    'user_gender': bool,\n",
            "    'user_id': string,\n",
            "    'user_occupation_label': ClassLabel(shape=(), dtype=int64, num_classes=22),\n",
            "    'user_occupation_text': string,\n",
            "    'user_rating': float32,\n",
            "    'user_zip_code': string,\n",
            "})\n",
            "Filtering tf dataset for user_id, movie_id and user_rating\n",
            "Creating a vocabulary for user_id (str -> int)\n",
            "Vocabulary of user_id's has size: 944\n",
            "Creating a vocabulary for movie_id (str -> int)\n",
            "Vocabulary of movie_id's has size: 1683\n",
            "Splitting the dataset into train, validation and test sets with sizes: (0.8, 0.1, 0.1)\n",
            "Absolute sizes => Train: 80000, Validation: 10000, Test: 10000\n",
            "Running regularized UV factorization...\n",
            "Epoch 1/2 completed.\n",
            "Epoch 2/2 completed.\n",
            "\n",
            "====== Metrics and Examples ======\n",
            "MSE without regularization: 23.28307342529297\n",
            "MSE with regularization: 23.399478912353516\n",
            "Performing grid search for hyperparameters...\n",
            "Testing learning_rate: 0.009999999776482582, reg_param: 0.10000000149011612\n",
            "Epoch 1/2 completed.\n",
            "Epoch 2/2 completed.\n",
            "Validation Loss: 23.373571395874023\n",
            "Testing learning_rate: 0.009999999776482582, reg_param: 0.30000001192092896\n",
            "Epoch 1/2 completed.\n",
            "Epoch 2/2 completed.\n",
            "Validation Loss: 23.34774398803711\n",
            "Testing learning_rate: 0.029999999329447746, reg_param: 0.10000000149011612\n",
            "Epoch 1/2 completed.\n",
            "Epoch 2/2 completed.\n",
            "Validation Loss: 23.250465393066406\n",
            "Testing learning_rate: 0.029999999329447746, reg_param: 0.30000001192092896\n",
            "Epoch 1/2 completed.\n",
            "Epoch 2/2 completed.\n",
            "Validation Loss: 23.184171676635742\n",
            "Best hyperparameters: Learning Rate = 0.029999999329447746, Regularization = 0.30000001192092896\n",
            "\n",
            "====== Metrics and Examples ======\n",
            "MSE without regularization: 23.28307342529297\n",
            "MSE with regularization: 23.192153930664062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2HBYM9ajZXtE"
      },
      "id": "2HBYM9ajZXtE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}